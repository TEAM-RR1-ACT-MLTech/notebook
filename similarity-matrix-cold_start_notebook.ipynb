{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as \nimport random\nfrom sklearn import preprocessing\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nimport os\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nfrom scipy.sparse import csr_matrix\nimport scipy as sp\nfrom gensim.models import Word2Vec\nimport gensim\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(font_scale=1)\nsns.set_style(\"white\")\npd.set_option('display.max_columns', 37)\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.display import display_html \nfrom IPython.core.display import HTML\nfrom collections import defaultdict\nfrom surprise.model_selection import train_test_split\n# Packages for model evaluation\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom time import time\n\n# Package to suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Packages for saving models\nimport pickle\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.impute import SimpleImputer\nimport scipy as sp\nfrom surprise import SVD\nfrom surprise import Dataset\nfrom surprise import Reader\nfrom gensim.models import Word2Vec\nfrom surprise.model_selection import cross_validate\nimport scipy.stats\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-24T06:56:57.438741Z","iopub.execute_input":"2023-07-24T06:56:57.439230Z","iopub.status.idle":"2023-07-24T06:57:00.068758Z","shell.execute_reply.started":"2023-07-24T06:56:57.439190Z","shell.execute_reply":"2023-07-24T06:57:00.067608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:57:28.125954Z","iopub.execute_input":"2023-07-24T06:57:28.126448Z","iopub.status.idle":"2023-07-24T06:57:28.152001Z","shell.execute_reply.started":"2023-07-24T06:57:28.126415Z","shell.execute_reply":"2023-07-24T06:57:28.150842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/edsa-movie-recommender/sample_submission.csv')\nmovies = pd.read_csv('/kaggle/input/edsa-movie-recommender/movies.csv')\nimdb_data = pd.read_csv('/kaggle/input/edsa-movie-recommender/imdb_data.csv')\ngenome_scores = pd.read_csv('/kaggle/input/edsa-movie-recommender/genome_scores.csv')\ngenome_tags = pd.read_csv('/kaggle/input/edsa-movie-recommender/genome_tags.csv')\ntrain = pd.read_csv('/kaggle/input/edsa-movie-recommender/train.csv')\ntest = pd.read_csv('/kaggle/input/edsa-movie-recommender/test.csv')\ntags = pd.read_csv('/kaggle/input/edsa-movie-recommender/tags.csv')\nlinks = pd.read_csv('/kaggle/input/edsa-movie-recommender/links.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:57:00.071326Z","iopub.execute_input":"2023-07-24T06:57:00.071741Z","iopub.status.idle":"2023-07-24T06:57:28.114732Z","shell.execute_reply.started":"2023-07-24T06:57:00.071700Z","shell.execute_reply":"2023-07-24T06:57:28.113605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of users\nprint('The ratings dataset has', train['userId'].nunique(), 'unique users')\n# Number of movies\nprint('The ratings dataset has', train['movieId'].nunique(), 'unique movies')\n# Number of ratings\nprint('The ratings dataset has', train['rating'].nunique(), 'unique ratings')\n# List of unique ratings\nprint('The unique ratings are', sorted(train['rating'].unique()))","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:57:28.155948Z","iopub.execute_input":"2023-07-24T06:57:28.156299Z","iopub.status.idle":"2023-07-24T06:57:28.579658Z","shell.execute_reply.started":"2023-07-24T06:57:28.156270Z","shell.execute_reply":"2023-07-24T06:57:28.578458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge ratings and movies datasets\ndf = pd.merge(train, movies, on='movieId', how='inner')\n# Take a look at the data\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:57:28.581208Z","iopub.execute_input":"2023-07-24T06:57:28.581953Z","iopub.status.idle":"2023-07-24T06:57:30.806313Z","shell.execute_reply.started":"2023-07-24T06:57:28.581910Z","shell.execute_reply":"2023-07-24T06:57:30.805131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aggregate by movie\nagg_ratings = df.groupby('title').agg(mean_rating = ('rating', 'mean'),\n                                                number_of_ratings = ('rating', 'count')).reset_index()\n# Keep the movies with over 100 ratings\nagg_ratings_GT100 = agg_ratings[agg_ratings['number_of_ratings']>100]\nagg_ratings_GT100.info()","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:57:30.807709Z","iopub.execute_input":"2023-07-24T06:57:30.808031Z","iopub.status.idle":"2023-07-24T06:57:32.362009Z","shell.execute_reply.started":"2023-07-24T06:57:30.808002Z","shell.execute_reply":"2023-07-24T06:57:32.360889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check popular movies\nagg_ratings_GT100.sort_values(by='number_of_ratings', ascending=False).head()","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:57:32.363595Z","iopub.execute_input":"2023-07-24T06:57:32.364023Z","iopub.status.idle":"2023-07-24T06:57:32.379010Z","shell.execute_reply.started":"2023-07-24T06:57:32.363981Z","shell.execute_reply":"2023-07-24T06:57:32.377788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visulization\nsns.jointplot(x='mean_rating', y='number_of_ratings', data=agg_ratings_GT100)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:57:32.380428Z","iopub.execute_input":"2023-07-24T06:57:32.380777Z","iopub.status.idle":"2023-07-24T06:57:34.271556Z","shell.execute_reply.started":"2023-07-24T06:57:32.380746Z","shell.execute_reply":"2023-07-24T06:57:34.270541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge data\ndf_GT100 = pd.merge(df, agg_ratings_GT100[['title']], on='title', how='inner')\ndf_GT100.info()","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:57:34.277641Z","iopub.execute_input":"2023-07-24T06:57:34.278422Z","iopub.status.idle":"2023-07-24T06:57:37.187165Z","shell.execute_reply.started":"2023-07-24T06:57:34.278374Z","shell.execute_reply":"2023-07-24T06:57:37.185996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of users\nprint('The ratings dataset has', df_GT100['userId'].nunique(), 'unique users')\n# Number of movies\nprint('The ratings dataset has', df_GT100['movieId'].nunique(), 'unique movies')\n# Number of ratings\nprint('The ratings dataset has', df_GT100['rating'].nunique(), 'unique ratings')\n# List of unique ratings\nprint('The unique ratings are', sorted(df_GT100['rating'].unique()))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:57:37.188560Z","iopub.execute_input":"2023-07-24T06:57:37.188890Z","iopub.status.idle":"2023-07-24T06:57:37.561481Z","shell.execute_reply.started":"2023-07-24T06:57:37.188861Z","shell.execute_reply":"2023-07-24T06:57:37.560344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\n\n# Calculate the number of rows in the DataFrame\nn = df_GT100.shape[0]\n\n# Calculate the sample size (0.01 percent of the original\ns = n // 10000\n\n# Generate the list of row indices to skip while sampling the DataFrame\nskip_indices = random.sample(range(n), n - s)\n\n# Sample the DataFrame by dropping the rows specified by the skip_indices list\nsampled_df = df_GT100.drop(skip_indices)\n\n# Number of users in the sampled dataframe\nprint('The sampled dataframe has', sampled_df['userId'].nunique(), 'unique users')\n\n# Number of movies in the sampled dataframe\nprint('The sampled dataframe has', sampled_df['movieId'].nunique(), 'unique movies')\n\n# Now you have a sampled dataframe with 0.1 percent of the original data, and you can proceed with your analysis using this sampled data.","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:57:37.562931Z","iopub.execute_input":"2023-07-24T06:57:37.563283Z","iopub.status.idle":"2023-07-24T06:57:55.977103Z","shell.execute_reply.started":"2023-07-24T06:57:37.563253Z","shell.execute_reply":"2023-07-24T06:57:55.975979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled_df.tail(50)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T08:25:06.310565Z","iopub.execute_input":"2023-07-24T08:25:06.311079Z","iopub.status.idle":"2023-07-24T08:25:06.343856Z","shell.execute_reply.started":"2023-07-24T08:25:06.311042Z","shell.execute_reply":"2023-07-24T08:25:06.342677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create user-item matrix\nmatrix = sampled_df.pivot_table(index='userId', columns='title', values='rating')","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:57:55.994990Z","iopub.execute_input":"2023-07-24T06:57:55.995441Z","iopub.status.idle":"2023-07-24T06:57:56.509724Z","shell.execute_reply.started":"2023-07-24T06:57:55.995400Z","shell.execute_reply":"2023-07-24T06:57:56.508330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:57:56.511554Z","iopub.execute_input":"2023-07-24T06:57:56.511893Z","iopub.status.idle":"2023-07-24T06:57:56.944145Z","shell.execute_reply.started":"2023-07-24T06:57:56.511863Z","shell.execute_reply":"2023-07-24T06:57:56.943069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize user-item matrix\nmatrix_norm = matrix.subtract(matrix.mean(axis=1), axis = 'rows')","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:57:56.945482Z","iopub.execute_input":"2023-07-24T06:57:56.946373Z","iopub.status.idle":"2023-07-24T06:57:57.245786Z","shell.execute_reply.started":"2023-07-24T06:57:56.946320Z","shell.execute_reply":"2023-07-24T06:57:57.244770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# User similarity matrix using cosine similarity\nuser_similarity_cosine = cosine_similarity(matrix_norm.fillna(0))","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:57:57.246974Z","iopub.execute_input":"2023-07-24T06:57:57.247297Z","iopub.status.idle":"2023-07-24T06:58:01.695396Z","shell.execute_reply.started":"2023-07-24T06:57:57.247269Z","shell.execute_reply":"2023-07-24T06:58:01.694151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# User similarity matrix using Pearson correlation\nuser_similarity = matrix_norm.T.corr()\nuser_similarity.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-24T06:58:01.697178Z","iopub.execute_input":"2023-07-24T06:58:01.697630Z","iopub.status.idle":"2023-07-24T07:00:24.764509Z","shell.execute_reply.started":"2023-07-24T06:58:01.697586Z","shell.execute_reply":"2023-07-24T07:00:24.763311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_similarity_cosine","metadata":{"execution":{"iopub.status.busy":"2023-07-24T07:56:08.566346Z","iopub.execute_input":"2023-07-24T07:56:08.566813Z","iopub.status.idle":"2023-07-24T07:56:08.577465Z","shell.execute_reply.started":"2023-07-24T07:56:08.566778Z","shell.execute_reply":"2023-07-24T07:56:08.575687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Find the number of unique values in the user similarity matrix\nnum_unique_values = user_similarity.nunique().sum()\n\nprint(\"Number of unique values in the user similarity matrix:\", num_unique_values)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T07:00:24.768073Z","iopub.execute_input":"2023-07-24T07:00:24.768431Z","iopub.status.idle":"2023-07-24T07:00:26.822562Z","shell.execute_reply.started":"2023-07-24T07:00:24.768401Z","shell.execute_reply":"2023-07-24T07:00:26.821376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transpose the data frame matrix (swap rows and columns)\ndf_transpose = matrix.transpose()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"users = list(df_transpose.columns)\nmovies = list(df_transpose.index)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_rated_user_for_a_movie(df_transpose: pd.DataFrame, movie: str):\n    return df_transpose.loc[title, :].dropna().index.values\n\n\ndef get_top_neighbors(\n    user_similarity: pd.DataFrame, user: str, rated_users: str, n_neighbors: int\n):\n    return user_similarity[user][rated_users].nlargest(n_neighbors).to_dict()\n\n\ndef subtract_bias(rating: float, mean_rating: float):\n    return rating - mean_rating\n\n\ndef get_neighbor_rating_without_bias_per_movie(\n    ratings_df: pd.DataFrame, user: str, movie: str\n):\n    \"\"\"Substract the rating of a user from the mean rating of that user to eliminate bias\"\"\"\n    mean_rating = ratings_df[user].mean()\n    rating = df_transpose.loc[movie, user]\n    return subtract_bias(rating, mean_rating)\n\n\ndef get_ratings_of_neighbors(df_transpose: pd.DataFrame, neighbors: list, movie: str):\n    \"\"\"Get the ratings of all neighbors after adjusting for biases\"\"\"\n    return [\n        get_neighbor_rating_without_bias_per_movie(df_transpose, neighbor, movie)\n        for neighbor in neighbors\n    ]\n\ndef get_weighted_average_rating_of_neighbors(ratings: list, neighbor_distance: list):\n    weighted_sum = np.array(ratings).dot(np.array(neighbor_distance))\n    abs_neigbor_distance = np.abs(neighbor_distance)\n    return weighted_sum / np.sum(abs_neigbor_distance)\n\n\ndef ger_user_rating(df_transpose: pd.DataFrame, user: str, avg_neighbor_rating: float):\n    user_avg_rating = df_transpose[user].mean()\n    return round(user_avg_rating + avg_neighbor_rating, 2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_rating(\n    df: pd.DataFrame,\n    user_similarity: pd.DataFrame,\n    user: str,\n    movie: str,\n    n_neighbors: int = 2,\n):\n    \"\"\"Predict the rating of a user for a movie based on the ratings of neighbors\"\"\"\n    df_transpose = df.copy()\n\n    rated_users = get_rated_user_for_a_movie(df_transpose, title)\n\n    top_neighbors_distance = get_top_neighbors(\n        similarity_df, user, rated_users, n_neighbors\n    )\n    neighbors, distance = top_neighbors_distance.keys(), top_neighbors_distance.values()\n\n    print(f\"Top {n_neighbors} neighbors of user {user}, {title}: {list(neighbors)}\")\n\n    ratings = get_ratings_of_neighbors(df_transpose, neighbors, title)\n    avg_neighbor_rating = get_weighted_average_rating_of_neighbors(\n        ratings, list(distance)\n    )\n\n    return ger_user_rating(df_transpose, user, avg_neighbor_rating)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the df_GT100 to a Surprise Dataset\nreader = Reader(rating_scale=(0, 5))\ndata = Dataset.load_from_df(df_GT100[['userId', 'movieId', 'rating']], reader)\n\n# Split the data into train and test sets (80% for training, 20% for testing)\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Build and train the SVD model on the train_data\nsvd_model = SVD(n_factors=100, random_state=42)\nsvd_model.fit(train_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef predict_rating_svd(svd_model, user, movie):\n    \"\"\"Predict the rating of a user for a movie using SVD\"\"\"\n    prediction = svd_model.predict(user, movie)\n    return prediction.est\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_rating_combined(df, user_similarity, svd_model, user, movie, n_neighbors=2):\n    \"\"\"Predict the rating of a user for a movie using a combination of user-based collaborative filtering and SVD\"\"\"\n    rated_users = get_rated_user_for_a_movie(df_transpose, movie)\n\n    top_neighbors_distance = get_top_neighbors(\n        user_similarity, user, rated_users, n_neighbors\n    )\n    neighbors, distance = top_neighbors_distance.keys(), top_neighbors_distance.values()\n\n    print(f\"Top {n_neighbors} neighbors of user {user} for {movie}: {list(neighbors)}\")\n\n    ratings = get_ratings_of_neighbors(df_transpose, neighbors, movie)\n    avg_neighbor_rating = get_weighted_average_rating_of_neighbors(\n        ratings, list(distance)\n    )\n\n    # Predict rating using SVD for the same movie\n    svd_rating = predict_rating_svd(svd_model, user, movie)\n\n    # Combine user-based CF and SVD predictions\n    combined_rating = (0.7 * avg_neighbor_rating) + (0.3 * svd_rating)\n\n    return ger_user_rating(df_transpose, user, combined_rating)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_rating_combined(df, user_similarity, svd_model, user, movie, n_neighbors=2):\n    \"\"\"Predict the rating of a user for a movie using a combination of user-based collaborative filtering and SVD\"\"\"\n    rated_users = get_rated_user_for_a_movie(df_transpose, movie)\n\n    top_neighbors_distance = get_top_neighbors(\n        user_similarity, user, rated_users, n_neighbors\n    )\n    neighbors, distance = top_neighbors_distance.keys(), top_neighbors_distance.values()\n\n    print(f\"Top {n_neighbors} neighbors of user {user} for {movie}: {list(neighbors)}\")\n\n    ratings = get_ratings_of_neighbors(df_transpose, neighbors, movie)\n    avg_neighbor_rating = get_weighted_average_rating_of_neighbors(\n        ratings, list(distance)\n    )\n\n    # Predict rating using SVD for the same movie\n    svd_rating = predict_rating_svd(svd_model, user, movie)\n\n    # Combine user-based CF and SVD predictions\n    combined_rating = (0.7 * avg_neighbor_rating) + (0.3 * svd_rating)\n\n    return ger_user_rating(df_transpose, user, combined_rating)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Iterate through each row in the test dataframe\nfor index, row in test.iterrows():\n    user = str(row['userId'])\n    movie = str(row['movieId'])\n    # Predict the rating using the combined prediction function\n    rating = predict_rating_combined(df_GT100, user_similarity, svd_model, user, movie)\n    # Append the user, movie, and predicted rating to the list\n    predicted_ratings.append([user, movie, rating])\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame from the predicted ratings list\nsubmission_df = pd.DataFrame(predicted_ratings, columns=['userId', 'movieId', 'rating'])\n\n# Save the submission DataFrame to a CSV file for submission to Kaggle\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}